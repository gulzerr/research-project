{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d093b7b",
   "metadata": {},
   "source": [
    "# Fog Classification Training Pipeline\n",
    "\n",
    "This notebook trains a ResNet-50 classifier for fog detection using FADE-based weak labels.\n",
    "\n",
    "**Dataset Split**: 80% Train, 20% Test\n",
    "\n",
    "**Classes**: \n",
    "- Clear (score < 1.0)\n",
    "- Light Fog (1.0 ≤ score < 2.0)  \n",
    "- Dense Fog (score ≥ 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010b46fa",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bd9761b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from cnnClassifier import FogResNet50Classifier\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c055e0e",
   "metadata": {},
   "source": [
    "## 2. Define Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "004bf5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FogDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe: pandas DataFrame with columns ['image_id', 'image_path', 'score', 'weak_label']\n",
    "            transform: torchvision transforms\n",
    "        \"\"\"\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Map labels to integers\n",
    "        self.label_map = {'clear': 0, 'light_fog': 1, 'dense_fog': 2}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Use absolute path from parquet file\n",
    "        image_path = row['image_path']\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {image_path}: {e}\")\n",
    "            # Return a black image as fallback\n",
    "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Get label and normalized score (normalize to 0-1 range for regression head)\n",
    "        label = self.label_map[row['weak_label']]\n",
    "        # Normalize score: cap at max 3.0 and divide by 3\n",
    "        normalized_score = min(float(row['score']), 3.0) / 3.0\n",
    "        \n",
    "        return image, label, normalized_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b509c5",
   "metadata": {},
   "source": [
    "## 3. Define Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b6faead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(is_training=True):\n",
    "    \"\"\"Get data augmentation transforms\"\"\"\n",
    "    if is_training:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.RandomCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adec5e1f",
   "metadata": {},
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "569897eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weak_labels(df):\n",
    "    \"\"\"\n",
    "    Create weak labels from FADE scores.\n",
    "    Thresholds: clear (0-1), light_fog (1-2), dense_fog (>2)\n",
    "    \"\"\"\n",
    "    if 'weak_label' in df.columns:\n",
    "        print(\"Weak labels already exist in dataframe\")\n",
    "        return df\n",
    "    \n",
    "    if 'score' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain 'score' column\")\n",
    "    \n",
    "    print(\"Creating weak labels from FADE scores...\")\n",
    "    print(f\"Score range: {df['score'].min():.2f} - {df['score'].max():.2f}\")\n",
    "    \n",
    "    def score_to_label(score):\n",
    "        if score < 1.0:\n",
    "            return 'clear'\n",
    "        elif score < 2.0:\n",
    "            return 'light_fog'\n",
    "        else:\n",
    "            return 'dense_fog'\n",
    "    \n",
    "    df['weak_label'] = df['score'].apply(score_to_label)\n",
    "    \n",
    "    print(\"\\nWeak labels created:\")\n",
    "    print(df['weak_label'].value_counts())\n",
    "    print(\"\\nScore distribution per label:\")\n",
    "    for label in ['clear', 'light_fog', 'dense_fog']:\n",
    "        if label in df['weak_label'].values:\n",
    "            scores = df[df['weak_label'] == label]['score']\n",
    "            print(f\"{label}: mean={scores.mean():.2f}, min={scores.min():.2f}, max={scores.max():.2f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_class_weights(df):\n",
    "    \"\"\"Calculate class weights for handling imbalance\"\"\"\n",
    "    label_counts = df['weak_label'].value_counts()\n",
    "    total = len(df)\n",
    "    \n",
    "    weights = {}\n",
    "    for label in ['clear', 'light_fog', 'dense_fog']:\n",
    "        if label in label_counts:\n",
    "            weights[label] = total / (len(label_counts) * label_counts[label])\n",
    "        else:\n",
    "            weights[label] = 1.0\n",
    "    \n",
    "    # Convert to tensor in order [clear, light_fog, dense_fog]\n",
    "    weight_tensor = torch.tensor([weights['clear'], weights['light_fog'], weights['dense_fog']], \n",
    "                                dtype=torch.float32)\n",
    "    \n",
    "    print(f\"Class distribution:\")\n",
    "    print(label_counts)\n",
    "    print(f\"\\nClass weights: {weight_tensor}\")\n",
    "    \n",
    "    return weight_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510060ed",
   "metadata": {},
   "source": [
    "## 5. Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "561dcc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, class_criterion, density_criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for images, labels, fade_scores in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        fade_scores = fade_scores.to(device).float().unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        class_logits, density_pred = model(images)\n",
    "        \n",
    "        # Calculate losses\n",
    "        class_loss = class_criterion(class_logits, labels)\n",
    "        density_loss = density_criterion(density_pred, fade_scores)\n",
    "        loss = class_loss + 0.5 * density_loss  # Combined loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(class_logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100*correct/total:.2f}%'})\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def validate(model, test_loader, class_criterion, density_criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_densities = []\n",
    "    all_fade_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, fade_scores in tqdm(test_loader, desc='Testing'):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            fade_scores = fade_scores.to(device).float().unsqueeze(1)\n",
    "            \n",
    "            class_logits, density_pred = model(images)\n",
    "            \n",
    "            class_loss = class_criterion(class_logits, labels)\n",
    "            density_loss = density_criterion(density_pred, fade_scores)\n",
    "            loss = class_loss + 0.5 * density_loss\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(class_logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Store predictions for analysis\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_densities.extend(density_pred.cpu().numpy())\n",
    "            all_fade_scores.extend(fade_scores.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    # Calculate correlation between predicted density and FADE scores\n",
    "    correlation = np.corrcoef(np.array(all_densities).flatten(), \n",
    "                             np.array(all_fade_scores).flatten())[0, 1]\n",
    "    \n",
    "    return avg_loss, accuracy, correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339721ae",
   "metadata": {},
   "source": [
    "## 6. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0fb0a0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Models will be saved to: ../models\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "PARQUET_FILE = '../data/fade_results_complete.parquet'\n",
    "MODELS_FOLDER = '../models'\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_WORKERS = 0  # Set to 0 for notebook compatibility (multiprocessing doesn't work in notebooks)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create models folder if it doesn't exist\n",
    "os.makedirs(MODELS_FOLDER, exist_ok=True)\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Models will be saved to: {MODELS_FOLDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b62557",
   "metadata": {},
   "source": [
    "## 7. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9f1060e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../data/fade_results_complete.parquet...\n",
      "Loaded 19811 samples\n",
      "Columns: ['image_id', 'image_path', 'score']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>foggy_012970.jpg</td>\n",
       "      <td>/Users/deb/Documents/projects/research-project...</td>\n",
       "      <td>0.889074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>foggy_006905.jpg</td>\n",
       "      <td>/Users/deb/Documents/projects/research-project...</td>\n",
       "      <td>0.423859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>foggy_019825.jpg</td>\n",
       "      <td>/Users/deb/Documents/projects/research-project...</td>\n",
       "      <td>3.375630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>foggy_017808.jpg</td>\n",
       "      <td>/Users/deb/Documents/projects/research-project...</td>\n",
       "      <td>1.053913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>foggy_010801.jpg</td>\n",
       "      <td>/Users/deb/Documents/projects/research-project...</td>\n",
       "      <td>3.503386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           image_id                                         image_path  \\\n",
       "0  foggy_012970.jpg  /Users/deb/Documents/projects/research-project...   \n",
       "1  foggy_006905.jpg  /Users/deb/Documents/projects/research-project...   \n",
       "2  foggy_019825.jpg  /Users/deb/Documents/projects/research-project...   \n",
       "3  foggy_017808.jpg  /Users/deb/Documents/projects/research-project...   \n",
       "4  foggy_010801.jpg  /Users/deb/Documents/projects/research-project...   \n",
       "\n",
       "      score  \n",
       "0  0.889074  \n",
       "1  0.423859  \n",
       "2  3.375630  \n",
       "3  1.053913  \n",
       "4  3.503386  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score statistics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    19811.000000\n",
       "mean         2.752235\n",
       "std          2.031054\n",
       "min          0.072503\n",
       "25%          1.136188\n",
       "50%          2.200730\n",
       "75%          3.851727\n",
       "max         12.177445\n",
       "Name: score, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "print(f\"Loading data from {PARQUET_FILE}...\")\n",
    "df = pd.read_parquet(PARQUET_FILE)\n",
    "print(f\"Loaded {len(df)} samples\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Show first few rows and statistics\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(f\"\\nScore statistics:\")\n",
    "display(df['score'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68ac3352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating weak labels from FADE scores...\n",
      "Score range: 0.07 - 12.18\n",
      "\n",
      "Weak labels created:\n",
      "weak_label\n",
      "dense_fog    10646\n",
      "light_fog     5098\n",
      "clear         4067\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Score distribution per label:\n",
      "clear: mean=0.70, min=0.07, max=1.00\n",
      "light_fog: mean=1.45, min=1.00, max=2.00\n",
      "dense_fog: mean=4.16, min=2.00, max=12.18\n"
     ]
    }
   ],
   "source": [
    "# Create weak labels\n",
    "df = create_weak_labels(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "099398a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train samples: 15848 (80%)\n",
      "Test samples: 3963 (20%)\n"
     ]
    }
   ],
   "source": [
    "# Split data into train (80%) and test (20%)\n",
    "min_samples_per_class = df['weak_label'].value_counts().min()\n",
    "\n",
    "if len(df) < 10:\n",
    "    print(f\"\\nWARNING: Very small dataset ({len(df)} samples)!\")\n",
    "    print(\"Using all data for both training and testing (no split).\")\n",
    "    train_df = df.copy()\n",
    "    test_df = df.copy()\n",
    "elif min_samples_per_class < 2:\n",
    "    print(f\"\\nWARNING: Some classes have fewer than 2 samples!\")\n",
    "    print(\"Splitting without stratification...\")\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "else:\n",
    "    # Normal split with stratification (80% train, 20% test)\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, \n",
    "                                        stratify=df['weak_label'])\n",
    "\n",
    "print(f\"\\nTrain samples: {len(train_df)} (80%)\")\n",
    "print(f\"Test samples: {len(test_df)} (20%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a00bd53",
   "metadata": {},
   "source": [
    "## 8. Create Datasets and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4883a218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "weak_label\n",
      "dense_fog    8516\n",
      "light_fog    4078\n",
      "clear        3254\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class weights: tensor([1.6234, 1.2954, 0.6203])\n",
      "\n",
      "Train batches: 496\n",
      "Test batches: 124\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weights for imbalance handling\n",
    "class_weights = calculate_class_weights(train_df).to(DEVICE)\n",
    "\n",
    "# Create datasets (image paths are already in the dataframe)\n",
    "train_dataset = FogDataset(train_df, transform=get_transforms(is_training=True))\n",
    "test_dataset = FogDataset(test_df, transform=get_transforms(is_training=False))\n",
    "\n",
    "# Create data loaders (num_workers=0 for notebook compatibility)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, \n",
    "                        shuffle=True, num_workers=NUM_WORKERS, pin_memory=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, \n",
    "                      shuffle=False, num_workers=NUM_WORKERS, pin_memory=False)\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268c9e6a",
   "metadata": {},
   "source": [
    "## 9. Initialize Model and Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57be5de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ResNet-50 model...\n",
      "✓ Model initialized with 25.08M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deb/Documents/projects/research-project/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/deb/Documents/projects/research-project/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "print(\"Initializing ResNet-50 model...\")\n",
    "model = FogResNet50Classifier(num_classes=3, pretrained=True).to(DEVICE)\n",
    "\n",
    "# Loss functions\n",
    "class_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "density_criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "print(f\"✓ Model initialized with {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5624bf3",
   "metadata": {},
   "source": [
    "## 10. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a27198d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "==================================================\n",
      "Epoch 1/20\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 496/496 [28:47<00:00,  3.48s/it, loss=0.6284, acc=81.85%]\n",
      "Testing: 100%|██████████| 124/124 [04:10<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Train Loss: 0.4695, Train Acc: 81.85%\n",
      "  Test Loss: 0.3102, Test Acc: 88.04%\n",
      "  Density Correlation: 0.9680\n",
      "  Learning Rate: 0.000099\n",
      "  ✓ Saved best model (Test Acc: 88.04%)\n",
      "\n",
      "==================================================\n",
      "Epoch 2/20\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 496/496 [28:04<00:00,  3.40s/it, loss=0.2030, acc=84.48%]\n",
      "Testing: 100%|██████████| 124/124 [04:10<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Train Loss: 0.3921, Train Acc: 84.48%\n",
      "  Test Loss: 0.3363, Test Acc: 85.47%\n",
      "  Density Correlation: 0.9710\n",
      "  Learning Rate: 0.000098\n",
      "\n",
      "==================================================\n",
      "Epoch 3/20\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 496/496 [28:01<00:00,  3.39s/it, loss=0.1274, acc=86.20%]\n",
      "Testing: 100%|██████████| 124/124 [04:10<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Train Loss: 0.3607, Train Acc: 86.20%\n",
      "  Test Loss: 0.3072, Test Acc: 86.32%\n",
      "  Density Correlation: 0.9730\n",
      "  Learning Rate: 0.000095\n",
      "\n",
      "==================================================\n",
      "Epoch 4/20\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 496/496 [32:57<00:00,  3.99s/it, loss=0.0379, acc=87.13%]\n",
      "Testing: 100%|██████████| 124/124 [10:06<00:00,  4.89s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Train Loss: 0.3397, Train Acc: 87.13%\n",
      "  Test Loss: 0.3162, Test Acc: 87.11%\n",
      "  Density Correlation: 0.9677\n",
      "  Learning Rate: 0.000090\n",
      "\n",
      "==================================================\n",
      "Epoch 5/20\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 496/496 [28:39<00:00,  3.47s/it, loss=0.3144, acc=88.19%]\n",
      "Testing: 100%|██████████| 124/124 [04:12<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Train Loss: 0.3117, Train Acc: 88.19%\n",
      "  Test Loss: 0.2853, Test Acc: 88.01%\n",
      "  Density Correlation: 0.9710\n",
      "  Learning Rate: 0.000085\n",
      "\n",
      "==================================================\n",
      "Epoch 6/20\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 496/496 [28:10<00:00,  3.41s/it, loss=0.4099, acc=88.41%]\n",
      "Testing: 100%|██████████| 124/124 [04:13<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Train Loss: 0.2961, Train Acc: 88.41%\n",
      "  Test Loss: 0.3216, Test Acc: 85.42%\n",
      "  Density Correlation: 0.9670\n",
      "  Learning Rate: 0.000079\n",
      "\n",
      "==================================================\n",
      "Epoch 7/20\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 496/496 [28:11<00:00,  3.41s/it, loss=1.2863, acc=89.56%]\n",
      "Testing: 100%|██████████| 124/124 [04:15<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Train Loss: 0.2778, Train Acc: 89.56%\n",
      "  Test Loss: 0.3116, Test Acc: 86.58%\n",
      "  Density Correlation: 0.9728\n",
      "  Learning Rate: 0.000073\n",
      "\n",
      "==================================================\n",
      "Epoch 8/20\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 496/496 [28:09<00:00,  3.41s/it, loss=0.0310, acc=90.24%]\n",
      "Testing: 100%|██████████| 124/124 [04:13<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Train Loss: 0.2591, Train Acc: 90.24%\n",
      "  Test Loss: 0.3440, Test Acc: 87.81%\n",
      "  Density Correlation: 0.9721\n",
      "  Learning Rate: 0.000065\n",
      "\n",
      "==================================================\n",
      "Epoch 9/20\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 496/496 [28:32<00:00,  3.45s/it, loss=0.2384, acc=91.24%]\n",
      "Testing: 100%|██████████| 124/124 [04:22<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Train Loss: 0.2391, Train Acc: 91.24%\n",
      "  Test Loss: 0.3020, Test Acc: 87.48%\n",
      "  Density Correlation: 0.9742\n",
      "  Learning Rate: 0.000058\n",
      "\n",
      "==================================================\n",
      "Epoch 10/20\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/496 [00:07<59:02,  7.16s/it, loss=0.3066, acc=90.62%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mdensity_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Test\u001b[39;00m\n\u001b[1;32m     15\u001b[0m test_loss, test_acc, correlation \u001b[38;5;241m=\u001b[39m validate(model, test_loader, class_criterion, \n\u001b[1;32m     16\u001b[0m                                           density_criterion, DEVICE)\n",
      "Cell \u001b[0;32mIn[34], line 25\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, class_criterion, density_criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m class_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m density_loss  \u001b[38;5;66;03m# Combined loss\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Statistics\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/projects/research-project/.venv/lib/python3.9/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/research-project/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/research-project/.venv/lib/python3.9/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_test_acc = 0\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, class_criterion, \n",
    "                                       density_criterion, optimizer, DEVICE)\n",
    "    \n",
    "    # Test\n",
    "    test_loss, test_acc, correlation = validate(model, test_loader, class_criterion, \n",
    "                                              density_criterion, DEVICE)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "    print(f\"  Density Correlation: {correlation:.4f}\")\n",
    "    print(f\"  Learning Rate: {current_lr:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        model_path = os.path.join(MODELS_FOLDER, 'best_fog_resnet50.pth')\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'test_acc': test_acc,\n",
    "            'test_loss': test_loss,\n",
    "            'correlation': correlation,\n",
    "        }, model_path)\n",
    "        \n",
    "        print(f\"  ✓ Saved best model (Test Acc: {test_acc:.2f}%)\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best test accuracy: {best_test_acc:.2f}%\")\n",
    "print(f\"Model saved at: {os.path.join(MODELS_FOLDER, 'best_fog_resnet50.pth')}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859fc3b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
